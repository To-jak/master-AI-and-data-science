{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOlhXEfUsLzl",
        "colab_type": "text"
      },
      "source": [
        "# GPU programming: Parallel Reduction\n",
        "\n",
        "Implementation of different optimisation methods for parallel reduction of a large array with CUDA, as presented in [this presentation](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf) of Mark Harris.\n",
        "\n",
        "Implemented Kernels:\n",
        "* Interleaved addressing with high **divergence**\n",
        "* Interleaved addressing with **bank conflicts**\n",
        "* Sequential addressing without **bank conflicts**\n",
        "\n",
        "Also a version with **Thrust** library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzro8iIq_HmY",
        "colab_type": "text"
      },
      "source": [
        "Execute the cells below to run the code in google colab environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnidKocsE22w",
        "colab_type": "code",
        "outputId": "63c24cbb-b835-4c65-cb10-20de8430a201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "! pip install git+git://github.com/frehseg/nvcc4jupyter.git"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/frehseg/nvcc4jupyter.git\n",
            "  Cloning git://github.com/frehseg/nvcc4jupyter.git to /tmp/pip-req-build-9znw1z6_\n",
            "  Running command git clone -q git://github.com/frehseg/nvcc4jupyter.git /tmp/pip-req-build-9znw1z6_\n",
            "Requirement already satisfied (use --upgrade to upgrade): NVCCPlugin==0.0.1 from git+git://github.com/frehseg/nvcc4jupyter.git in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.1-cp36-none-any.whl size=2095 sha256=69694894dddb35da0df212cf4c368ed118ba5796147ea193ffde451ff3087e5a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nrh9r5rt/wheels/a4/a5/24/17a2b61f9a725a10155cc6fca753aae28436921df21fa16114\n",
            "Successfully built NVCCPlugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvvCJHsfFDDx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1b830216-287a-4dc2-86d3-9083ede78377"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1qMW8IKFxpQ",
        "colab_type": "code",
        "outputId": "8a0bd2c9-dd9c-4ab8-c340-cba119ad4165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <thrust/device_vector.h>\n",
        "#include <thrust/reduce.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "\n",
        "void print_array(char *chaine, float *tab, int len){\n",
        "   int k;\n",
        "   printf(\"\\nIn array: %i elements\", len);\n",
        "   printf(\"\\nLes 10 premiers de %s: \\n\",chaine);\n",
        "   for (k=0; k<10; k++) \n",
        "      printf(\"%.2f \",tab[k]);\n",
        "   printf(\"\\nLes 10 derniers: \\n\");\n",
        "   for (k=len-10; k<len; k++) \n",
        "      printf(\"%.2f \",tab[k]);\n",
        "   printf(\"\\n\");\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "enum reduce_type { DIVERGENT_BRANCH, BANK_CONFLICT, NO_BANK_CONFLICT };\n",
        "\n",
        "/* -------- KERNEL WITH DIVERGENT BRANCHING -------- */\n",
        "__global__ void reduce_kernel(float * d_out, float * d_in)\n",
        "{\n",
        "  extern __shared__ float sdata[];\n",
        " \n",
        "  unsigned int tid = threadIdx.x;\n",
        "  unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "  sdata[tid] = d_in[i];\n",
        "  __syncthreads();\n",
        " \n",
        "  for(unsigned int s = 1; s < blockDim.x; s *= 2){\n",
        "    if(tid % (2*s) == 0){\n",
        "        sdata[tid] += sdata[tid+s];\n",
        "    }\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  // only thread 0 writes result, as thread\n",
        "  if (tid == 0){\n",
        "    //printf(\"\\nblockIdx %i: %f\", blockIdx.x, sdata[tid]);\n",
        "    d_out[blockIdx.x] = sdata[0];\n",
        "  }\n",
        "}\n",
        "\n",
        "/* -------- KERNEL WITH BANK CONFLICTS -------------- */\n",
        "__global__ void reduce_kernel_bank_conflicts(float * d_out, float * d_in)\n",
        "{\n",
        "  extern __shared__ float sdata[];\n",
        " \n",
        "  unsigned int tid = threadIdx.x;\n",
        "  unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "  sdata[tid] = d_in[i];\n",
        "  __syncthreads();\n",
        " \n",
        "  for (unsigned int s=1; s < blockDim.x; s *= 2) {\n",
        "    int index = 2 * s * tid;\n",
        "    if (index < blockDim.x) {\n",
        "      sdata[index] += sdata[index + s];\n",
        "    }\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  // only thread 0 writes result, as thread\n",
        "  if (tid == 0){\n",
        "    //printf(\"\\nblockIdx %i: %f\", blockIdx.x, sdata[tid]);\n",
        "    d_out[blockIdx.x] = sdata[0];\n",
        "  }\n",
        "}\n",
        "\n",
        "/* -------- KERNEL WITHOUT BANK CONFLICTS -------------- */\n",
        "__global__ void reduce_kernel_without_bank_conflicts(float * d_out, float * d_in)\n",
        "{\n",
        "  extern __shared__ float sdata[];\n",
        " \n",
        "  unsigned int tid = threadIdx.x;\n",
        "  unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "  sdata[tid] = d_in[i];\n",
        "  __syncthreads();\n",
        " \n",
        "  for (unsigned int s=blockDim.x/2; s>0; s>>=1) {\n",
        "    if (tid < s) {\n",
        "      sdata[tid] += sdata[tid + s];\n",
        "    }\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  // only thread 0 writes result, as thread\n",
        "  if (tid == 0){\n",
        "    //printf(\"\\nblockIdx %i: %f\", blockIdx.x, sdata[tid]);\n",
        "    d_out[blockIdx.x] = sdata[0];\n",
        "  }\n",
        "}\n",
        "\n",
        "void apply_kernel(int num_blocks, int num_threads, float * d_out, float * d_in, reduce_type reduce_flag)\n",
        "{\n",
        "  switch (reduce_flag)\n",
        "  {\n",
        "      case DIVERGENT_BRANCH:\n",
        "        reduce_kernel<<<num_blocks, num_threads, sizeof(float)*num_threads>>>(d_out, d_in);\n",
        "        break;\n",
        "      case BANK_CONFLICT:\n",
        "        reduce_kernel_bank_conflicts<<<num_blocks, num_threads, sizeof(float)*num_threads>>>(d_out, d_in);\n",
        "        break;\n",
        "      case NO_BANK_CONFLICT:\n",
        "        reduce_kernel_without_bank_conflicts<<<num_blocks, num_threads, sizeof(float)*num_threads>>>(d_out, d_in);\n",
        "        break;\n",
        "      default:\n",
        "        reduce_kernel<<<num_blocks, num_threads, sizeof(float)*num_threads>>>(d_out, d_in);\n",
        "  }\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "/* -------- RECURSIVE KERNEL REDUCTION -------------- */\n",
        "void recursive_reduction(float* d_out, float* d_in, const int size_in, const int num_threads, reduce_type reduce_flag)\n",
        "{\n",
        "  switch (reduce_flag)\n",
        "  {\n",
        "      case DIVERGENT_BRANCH:\n",
        "        printf(\"\\n---- Reduction with divergent branching ----\\n\");\n",
        "        break;\n",
        "      case BANK_CONFLICT:\n",
        "        printf(\"\\n---- Reduction with bank conflicts ----\\n\");\n",
        "        break;\n",
        "      case NO_BANK_CONFLICT:\n",
        "        printf(\"\\n---- Reduction without bank conflicts ----\\n\");\n",
        "        break;\n",
        "      default:\n",
        "        printf(\"\\n---- Reduction with divergent branching ----\\n\");\n",
        "  }\n",
        " \n",
        "  // Setting up initial number of blocks\n",
        "  int num_blocks = ((size_in-1) / num_threads) + 1;\n",
        "  int prev_num_blocks = size_in;\n",
        "\n",
        "  // To store intermediate input and output array\n",
        "  // we define two stuctures.\n",
        "  // We will need to update them between kernel operations\n",
        "  float * d_in_intermediate;\n",
        "  cudaMalloc(&d_in_intermediate, sizeof(float)*size_in);\n",
        "  cudaMemcpy(d_in_intermediate, d_in, sizeof(float)*size_in, cudaMemcpyDeviceToDevice);\n",
        " \n",
        "  // Setting up intermediate ouput structure\n",
        "  // to store intermediate results between recursive kernel calls.\n",
        "  // Each block compute one element of intermediate result\n",
        "  int extended_size_out = ( ((num_blocks-1)/num_threads) + 1 ) * num_threads;\n",
        "  float * d_out_intermediate;\n",
        "  cudaMalloc(&d_out_intermediate, sizeof(float)*extended_size_out);\n",
        "  cudaMemset(d_out_intermediate, 0, sizeof(float)*extended_size_out);\n",
        " \n",
        "  printf(\"\\nInitial number of blocks: %i\", num_blocks);\n",
        "  // recursively solving, will run approximately log base num_threads times.\n",
        "  do\n",
        "  {\n",
        "    apply_kernel(num_blocks, num_threads, d_out_intermediate, d_in_intermediate, reduce_flag);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Updating input to intermediate results for recursion\n",
        "    // Intermediate output become new intermediate input\n",
        "    cudaFree(d_in_intermediate);\n",
        "    cudaMalloc(&d_in_intermediate, sizeof(float)*extended_size_out);\n",
        "    cudaMemcpy(d_in_intermediate, d_out_intermediate, sizeof(float)*extended_size_out, cudaMemcpyDeviceToDevice);\n",
        "\n",
        "    prev_num_blocks = num_blocks;\n",
        "   \n",
        "    // New number of blocks required for next recursion\n",
        "    num_blocks = (num_blocks-1) / num_threads + 1;\n",
        "    if(num_blocks > 1 || prev_num_blocks > 1) printf(\"\\n\\nNext reduce number of blocks: %i\", num_blocks);\n",
        "   \n",
        "    // New extended size for intermediate output\n",
        "    extended_size_out = ( ((num_blocks-1)/num_threads) + 1 ) * num_threads;\n",
        "   \n",
        "    // Updating intermediate structure\n",
        "    cudaFree(d_out_intermediate);\n",
        "    cudaMalloc(&d_out_intermediate, sizeof(float)*extended_size_out);\n",
        "    cudaMemset(d_out_intermediate, 0, sizeof(float)*extended_size_out);\n",
        "  }\n",
        "  while(num_blocks > 1);\n",
        "  \n",
        "  // computing rest\n",
        "  if(prev_num_blocks > 1)\n",
        "  apply_kernel(1, num_threads, d_out, d_in_intermediate, reduce_flag);\n",
        "  else\n",
        "  cudaMemcpy(d_out, d_in_intermediate, sizeof(float), cudaMemcpyDeviceToDevice);\n",
        "  \n",
        "  cudaFree(d_in_intermediate);\n",
        "  cudaFree(d_out_intermediate);\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "/* -------- Reduction with THRUST -------- */\n",
        "void thrust_reduce(float* h_in, const int size_in)\n",
        "{\n",
        "  printf(\"\\n---- Reduction with Thrust ----\\n\");\n",
        " \n",
        "  thrust::device_vector<float> D (h_in, h_in+size_in);\n",
        "  float sum_t;\n",
        " \n",
        "  clock_t start = clock();\n",
        "  sum_t = thrust::reduce(D.begin(), D.end());\n",
        "  clock_t end = clock();\n",
        "  printf(\"\\nReduction time: %.f µs\\n\", (double) (end-start));\n",
        "  printf(\"\\nSum: %.f\\n\", sum_t);\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "/* -------- MAIN -------- */\n",
        "int main(void)\n",
        "{\n",
        "  // Setting num_threads\n",
        "  const int num_threads = 128;\n",
        " \n",
        "  // Setting input array\n",
        "  const int size_in = pow(2, 22) + 1;\n",
        " \n",
        "  float * h_in = (float *)malloc(size_in*sizeof(float));\n",
        "  for (int i = 0; i < size_in; i++) h_in[i] = 1.0f;\n",
        " \n",
        "  print_array(\"h_in\", h_in, size_in);\n",
        "\n",
        "  // Setting up an extended size if input array is not a multiple of num_threads\n",
        "  // We will extend the memory on the gpu initialized with value of 0 \n",
        "  int extended_size_in = ( ((size_in-1)/num_threads) + 1 ) * num_threads;\n",
        " \n",
        "  // Memory allocation on device for input and ouput\n",
        "  float * d_in;\n",
        "  cudaMalloc(&d_in, sizeof(float)*extended_size_in);\n",
        "  cudaMemset(d_in, 0, sizeof(float)*extended_size_in);\n",
        " \n",
        "  float * d_out;\n",
        "  cudaMalloc((void**)&d_out, sizeof(float));\n",
        "\n",
        "  // Copy input on device \n",
        "  cudaMemcpy(d_in, h_in, sizeof(float)*size_in, cudaMemcpyHostToDevice);\n",
        " \n",
        "  // Reduction with recursive kernel calls\n",
        "  clock_t start = clock();\n",
        "  recursive_reduction(d_out, d_in, extended_size_in, num_threads, DIVERGENT_BRANCH);\n",
        "  clock_t end = clock();\n",
        "  printf(\"\\n\\nReduction time: %.f µs\\n\", (double) (end-start));\n",
        " \n",
        "  start = clock();\n",
        "  recursive_reduction(d_out, d_in, extended_size_in, num_threads, BANK_CONFLICT);\n",
        "  end = clock();\n",
        "  printf(\"\\n\\nReduction time: %.f µs\\n\", (double) (end-start));\n",
        " \n",
        "  start = clock();\n",
        "  recursive_reduction(d_out, d_in, extended_size_in, num_threads, NO_BANK_CONFLICT);\n",
        "  end = clock();\n",
        "  printf(\"\\n\\nReduction time: %.f µs\\n\", (double) (end-start));\n",
        " \n",
        "  float result;\n",
        "  cudaMemcpy(&result, d_out, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "  printf(\"\\n\\nTotal sum of elements in array is: %.f\\n\", result);\n",
        " \n",
        "  // Version with thrust\n",
        "  thrust_reduce(h_in, size_in);\n",
        " \n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  free(h_in);\n",
        "}"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "In array: 4194305 elements\n",
            "Les 10 premiers de h_in: \n",
            "1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 \n",
            "Les 10 derniers: \n",
            "1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 \n",
            "\n",
            "---- Reduction with divergent branching ----\n",
            "\n",
            "Initial number of blocks: 32769\n",
            "\n",
            "Next reduce number of blocks: 257\n",
            "\n",
            "Next reduce number of blocks: 3\n",
            "\n",
            "Next reduce number of blocks: 1\n",
            "\n",
            "Reduction time: 1509 µs\n",
            "\n",
            "---- Reduction with bank conflicts ----\n",
            "\n",
            "Initial number of blocks: 32769\n",
            "\n",
            "Next reduce number of blocks: 257\n",
            "\n",
            "Next reduce number of blocks: 3\n",
            "\n",
            "Next reduce number of blocks: 1\n",
            "\n",
            "Reduction time: 1245 µs\n",
            "\n",
            "---- Reduction without bank conflicts ----\n",
            "\n",
            "Initial number of blocks: 32769\n",
            "\n",
            "Next reduce number of blocks: 257\n",
            "\n",
            "Next reduce number of blocks: 3\n",
            "\n",
            "Next reduce number of blocks: 1\n",
            "\n",
            "Reduction time: 1189 µs\n",
            "\n",
            "\n",
            "Total sum of elements in array is: 4194305\n",
            "\n",
            "---- Reduction with Thrust ----\n",
            "\n",
            "Reduction time: 168 µs\n",
            "\n",
            "Sum: 4194305\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}