{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Analyse de sentiments dans les critiques de films\n",
    "\n",
    "### Thomas Jacquemin MS IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "1. Implémenter une manière simple de représenter des données textuelles\n",
    "2. Implémenter un modèle d'apprentissage statistique basique\n",
    "3. Utiliser ces représentations et ce modèle pour une tâche d'analyse de sentiments\n",
    "4. Tenter d'améliorer les résultats avec des outils venus du traitement automatique du langage\n",
    "5. Comparer les résultats avec une l'implémentation de Scikit-Learn, et avec d'autres méthodes de représentation ou d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dépendances nécessaires\n",
    "\n",
    "Pour les objectifs 4. et 5., on aura besoin des packages suivants:\n",
    "- The Machine Learning API Scikit-learn : http://scikit-learn.org/stable/install.html\n",
    "- The Natural Language Toolkit : http://www.nltk.org/install.html\n",
    "\n",
    "Les deux sont disponibles avec Anaconda: https://anaconda.org/anaconda/nltk et https://anaconda.org/anaconda/scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import re \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger les données\n",
    "\n",
    "Extraire les données et placer le dossier 'data' dans le même dossier que le notebook.\n",
    "\n",
    "On récupère les données textuelles dans la variable *texts*\n",
    "\n",
    "On récupère les labels dans la variable $y$ qui en contient *len(texts)* : $0$ indique que la critique correspondante est négative tandis que $1$ qu'elle est positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "# We get the files from the path: ./data/imdb1/neg for negative reviews, and ./data/imdb1/pos for positive reviews\n",
    "filenames_neg = sorted(glob(op.join('.', 'data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('.', 'data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "# Each files contains a review that consists in one line of text: we put this string in two lists, that we concatenate\n",
    "texts_neg = [open(f, encoding=\"utf8\").read() for f in filenames_neg]\n",
    "texts_pos = [open(f, encoding=\"utf8\").read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "\n",
    "# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n",
    "# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de documents: 2000\n"
     ]
    }
   ],
   "source": [
    "# This number of documents may be high for most computers: we can select a fraction of them (here, one in k)\n",
    "# Use an even number to keep the same number of positive and negative reviews\n",
    "k = 1\n",
    "texts_reduced = texts[0::k]\n",
    "y_reduced = y [0::k]\n",
    "\n",
    "print('Nombre de documents:', len(texts_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idée principale\n",
    "\n",
    "On dispose d'une critique étant en fait une liste de mots $s = (w_1, ..., w_N)$, et l'on cherche à trouver la classe associée $c$ - qui dans notre cas, peut-être $c = 0$ ou $c = 1$. L'objectif est donc de trouver pour chaque critique $s$ la classe $\\hat{c}$ maximisant la probabilité conditionelle **$P(c|s)$** : \n",
    "\n",
    "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)}$$\n",
    "\n",
    "**Hypothèse : P(s) est constante pour chaque classe** :\n",
    "\n",
    "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)} = \\underset{c}{\\mathrm{argmax}}\\,P(s|c)P(c)$$\n",
    "\n",
    "**Hypothèse naïve : les différentes variables (mots) d'une critique sont indépendantes entre elles** : \n",
    "\n",
    "$$P(s|c) = P(w_1, ..., w_N|c)=\\Pi_{i=1..N}P(w_i|c)$$\n",
    "\n",
    "On va donc pouvoir se servir des critiques annotées à notre disposition pour **estimer les probabilités $P(w|c)$ pour chaque mot $w$ étant donné les deux classes $c$**. Ces critiques vont nous permettre d'apprendre à évaluer la \"compatibilité\" entre les mots et classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vue générale\n",
    "\n",
    "### Entraînement: Estimer les probabilités\n",
    "\n",
    "Pour chaque mot $w$ du vocabulaire $V$, $P(w|c)$ est le nombre d'occurences de $w$ dans une critique ayant pour classe $c$, divisé par le nombre total d'occurences dans $c$. Si on note $T(w,c)$ ce nombre d'occurences, on obtient:\n",
    "\n",
    "$$P(w|c) = \\text{Fréquence de }w\\text{ dans }c = \\frac{T(w,c)}{\\sum_{w' \\in V} T(w',c)}$$\n",
    "\n",
    "### Test: Calcul des scores\n",
    "\n",
    "Pour faciliter les calculs et éviter les erreurs d'*underflow* et d'approximation, on utilise le \"log-sum trick\", et on passe l'équation en log-probabilités : \n",
    "\n",
    "$$\\hat{c} =  \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\, \\left[ \\mathrm{log}(P(c)) + \\sum_{i=1..N}log(P(w_i|c)) \\right]$$\n",
    "\n",
    "### Laplace smoothing (Lissage)\n",
    "\n",
    "Un mot qui n'apparaît pas dans un document a une probabilité nulle: cela va poser problème avec le logarithme. On garde donc une toute petite partie de la masse de probabilité qu'on redistribue avec le *Laplace smoothing*: \n",
    "\n",
    "$$P(w|c) = \\frac{T(w,c) + 1}{\\sum_{w' \\in V} T(w',c) + 1}$$\n",
    "\n",
    "Il existe d'autre méthodes de lissage, en général adaptées à d'autres applications plus complexes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation adaptée des documents\n",
    "\n",
    "Notre modèle statistique, comme la plupart des modèles appliqués aux données textuelles, utilise les comptes d'occurences de mots dans un document. Ainsi, une manière très pratique de représenter un document est d'utiliser un vecteur \"Bag-of-Words\" (BoW), contenant les comptes de chaque mot (indifférement de leur ordre d'apparition) dans le document. \n",
    "\n",
    "Si on considère l'ensemble de tous les mots apparaissant dans nos $T$ documents d'apprentissage, que l'on note $V$ (Vocabulaire), on peut créer **un index**, qui est une bijection associant à chaque mot $w$ un entier, qui sera sa position dans $V$. \n",
    "\n",
    "Ainsi, pour un document extrait d'un ensemble de documents contenant $|V|$ mots différents, une représentation BoW sera un vecteur de taille $|V|$, dont la valeur à l'indice d'un mot $w$ sera son nombre d'occurences dans le document. \n",
    "\n",
    "On peut utiliser la classe **CountVectorizer** de scikit-learn pour mieux comprendre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['avenue', 'boulevard', 'city', 'down', 'ran', 'the', 'walk', 'walked']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 2, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 1, 0, 2, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['I walked down down the boulevard',\n",
    "          'I walked down the avenue',\n",
    "          'I ran down the boulevard',\n",
    "          'I walk down the city',\n",
    "          'I walk down the the avenue']\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "Bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "Bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche d'abord la liste contenant les mots ordonnés selon leur indice (On note que les mots de 2 caractères ou moins ne sont pas pris en compte)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détail: entraînement\n",
    "\n",
    "L'idée est d'extraire le nombre d'occurences $T(w,c)$ de chaque mot $w$ pour chaque classe $c$, ce qui permettra de calculer la matrice de probabilités conditionelles $\\pmb{P}$ telle que: $$\\pmb{P}_{w,c} = P(w|c)$$\n",
    "\n",
    "Notons que le nombre d'occurences $T(w,c)$ peut être obtenu facilement à partir des représentations BoW de l'ensemble des documents.\n",
    "\n",
    "### Procédure:\n",
    "<img src=\"algo_train.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "## Détail: test\n",
    "\n",
    "Nous connaissons maintenant les probabilités conditionelles données par la matrice $\\pmb{P}$. \n",
    "Il faut maintenant obtenir $P(s|c)$ pour le document courant. Cette quantité s'obtient à l'aide d'un calcul simple impliquant la représentation BoW du document et $\\pmb{P}$.\n",
    "\n",
    "### Procédure:\n",
    "<img src=\"algo_test.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing du texte: obtenir les représentations BoW\n",
    "\n",
    "D'abord, il faut transformer les critiques sous forme de strings en une liste de mots. La tactique la plus simple consiste à diviser le string suivant les espaces, avec la commande:\n",
    "```text.split()```\n",
    "\n",
    "Mais il faut aussi faire attention à enlever les caractères particuliers qui pourraient ne pas avoir été nettoyés (comme les balises HTML si on a obtenu les données à partir de pages web). Puisque l'on va compter les mots, il faudra construire une liste des mots apparaissant dans nos données. Dans notre cas, on aimerait réduire cette liste et l'uniformiser (ignorer les majuscules, la ponctuation, et les mots les plus courts). \n",
    "On va donc utiliser une fonction adaptée à nos besoins - mais c'est un travail qu'il n'est en général pas nécessaire de faire, puisqu'il existe de nombreux outils déjà adaptés à la plupart des cas de figures. \n",
    "Pour le nettoyage du texte, il existe de nombreux scripts, basés sur différents outils (expressions régulières, par exemple) qui permettent de préparer des données. La division du texte en mots et la gestion de la ponctuation est gérée lors d'une étape appellée *tokenization*; si besoin, un package python comme le NLTK contient de nombreux *tokenizers* différents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walked', 'down', 'down', 'the', 'boulevard', 'walked', 'down', 'the', 'avenue', 'ran', 'down', 'the', 'boulevard', 'walk', 'down', 'the', 'city', 'walk', 'down', 'the', 'the', 'avenue']\n",
      "['I', 'walked', 'down', 'down', 'the', 'boulevard', '.', 'I', 'walked', 'down', 'the', 'avenue', '.', 'I', 'ran', 'down', 'the', 'boulevard', '.', 'I', 'walk', 'down', 'the', 'city', '.', 'I', 'walk', 'down', 'the', 'the', 'avenue', '.']\n"
     ]
    }
   ],
   "source": [
    "# We might want to clean the file with various strategies:\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Cleaning a document with:\n",
    "        - Lowercase        \n",
    "        - Removing numbers with regular expressions\n",
    "        - Removing punctuation with regular expressions\n",
    "        - Removing other artifacts\n",
    "    And separate the document into words by simply splitting at spaces\n",
    "    Params:\n",
    "        text (string): a sentence or a document\n",
    "    Returns:\n",
    "        tokens (list of strings): the list of tokens (word units) forming the document\n",
    "    \"\"\"        \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    text = REMOVE_PUNCT.sub(\"\", text)\n",
    "    # Remove small words (1 and 2 characters)\n",
    "    text = re.sub(r\"\\b\\w{1,2}\\b\", \"\", text)\n",
    "    # Remove HTML artifacts specific to the corpus we're going to work with\n",
    "    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    text = REPLACE_HTML.sub(\" \", text)\n",
    "    \n",
    "    tokens = text.split()        \n",
    "    return tokens\n",
    "\n",
    "# Or we might want to use an already-implemented tool. The NLTK package has a lot of very useful text processing tools, among them various tokenizers\n",
    "# Careful, NLTK was the first well-documented NLP package, but it might be outdated for some uses. Check the documentation !\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus_raw = \"I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.\"\n",
    "print(clean_and_tokenize(corpus_raw))\n",
    "print(word_tokenize(corpus_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction **à compléter**. Elle prend en entrée une liste de document (chacun sous la forme d'un string) et renvoie, comme dans l'exemple utilisant ```CountVectorizer```:\n",
    "- Un vocabulaire qui associe à chaque mot rencontré un index\n",
    "- Une matrice, dont les lignes représentent les documents et les colonnes les mots indexés par le vocabulaire. En position $(i,j)$, on devra avoir le nombre d'occurence du mot $j$ dans le document $i$.\n",
    "\n",
    "Le vocabulaire, qui était sous la forme d'une *liste* dans l'exemple précédent, pourra être renvoyé sous forme de *dictionnaire* dont les clés sont les mots et les valeurs les indices. Puisque le vocabulaire recense les mots du corpus sans se soucier de leur nombre d'occurences, on pourra le constituer à l'aide d'un ensemble (```set``` en python). \n",
    "On pourra bien sur utiliser la fonction ```clean_and_tokenize``` pour transformer les strings en liste de mots. \n",
    "##### Quelques pointeurs pour les débutants en Python : \n",
    "\n",
    "- ```my_list.append(value)``` : put the variable ```value``` at the end of the list ```my_list```\n",
    "\n",
    "-  ```words = set()``` : create a set, which is a list of unique values \n",
    "\n",
    "- ```words.union(my_list)``` : extend the set ```words```\n",
    "\n",
    "- ```dict(zip(keys, values)))``` : create a dictionnary\n",
    "\n",
    "- ```for k, text in enumerate(texts)``` : syntax for a loop with the index, ```texts``` begin a list (of texts !)\n",
    "\n",
    "- ```len(my-list)``` : length of the list ```my_list```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform texts in list of tokens\n",
    "    tokenized_texts = []\n",
    "    for text in texts:\n",
    "        tokenized_texts.append(clean_and_tokenize(text))\n",
    "    \n",
    "    # Creating words set\n",
    "    words = set()\n",
    "    for tokenized_text in tokenized_texts:\n",
    "        words.update(tokenized_text)\n",
    "    \n",
    "    # Creating vocabulary dictionnary\n",
    "    vocabulary = {}\n",
    "    for i, word in enumerate(words):\n",
    "        vocabulary[word] = i\n",
    "    \n",
    "    # Creating count matrix\n",
    "    counts = np.zeros((len(tokenized_texts), len(vocabulary)))\n",
    "    for text_index, tokenized_text in enumerate(tokenized_texts):\n",
    "        for token in tokenized_text:\n",
    "            word_index = vocabulary[token]\n",
    "            counts[text_index][word_index] += 1\n",
    "            \n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'walked': 0, 'ran': 1, 'city': 2, 'boulevard': 3, 'down': 4, 'walk': 5, 'avenue': 6, 'the': 7}\n",
      "[[1. 0. 0. 1. 2. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0. 1. 1.]\n",
      " [0. 1. 0. 1. 1. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "Voc, X = count_words(corpus)\n",
    "print(Voc)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes \n",
    "\n",
    "Classe vide : fonctions **à compléter** \n",
    "```python\n",
    "def fit(self, X, y)\n",
    "``` \n",
    "**Entraînement** : va apprendre un modèle statistique basés sur les représentations $X$ correspondant aux labels $y$.\n",
    "$X$ représente donc ici des représentations obtenues en sortie de count_words. On complète la fonction à l'aide de la procédure détaillée plus haut. Si il est possible de la suivre à la lettre, les représentations que l'on utilise nous permettre d'être bien plus efficace et d'éviter d'utiliser des boucles !\n",
    "\n",
    "\n",
    "Note: le lissage, effectué à la ligne $10$, ne se fait pas nécessairement avec un $1$, mais peut se faire avec une valeur positive $\\alpha$, qu'on peut implémenter comme argument de la classe ```NB```.\n",
    "\n",
    "```python\n",
    "def predict(self, X)\n",
    "```\n",
    "**Testing** : va renvoyer les labels prédits par le modèle pour d'autres représentations $X$.\n",
    "\n",
    "\n",
    "\n",
    "Pour faciliter la procédure, on prendra la moitié de la matrice $X$ obtenue plus haut pour entraîner le modèle, et l'autre moitié pour l'évaluer. **Important**: cette façon de procéder n'est pas réaliste: en général, on ne dispose que des données d'entraînement au moment de créer le vocabulaire et d'entraîner le modèle. Ainsi, il est possible que les données d'évaluation contiennent des mots *inconnus*. C'est quelque chose qu'on peut traiter facilement en dédiant un indice à tous les mots rencontrés qui ne sont pas contenus dans le vocabulaire - mais il existe de nombreuses méthodes plus complexes pour réussir à utiliser à bon escient ces mots que le modèle n'a pas rencontré à l'entraînement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quelques pointeurs pour les débutants en Python : \n",
    "\n",
    "Utiliser l'API Numpy pour travailler avec des tenseurs\n",
    "\n",
    "\n",
    "- ```X.shape``` : for a ```numpy.array```, return the dimension of the tensor\n",
    "\n",
    "- ```np.zeros((dim_1, dim_2,...))``` : create a tensor filled with zeros\n",
    "\n",
    "- ```np.sum(X, axis = n)``` : sum the tensor over the axis n\n",
    "\n",
    "- ```np.mean(X, axis = n)```\n",
    "\n",
    "- ```np.argmax(X, axis = n)```\n",
    "\n",
    "- ```np.log(X)```\n",
    "\n",
    "- ```np.dot(X_1, X_1)``` : Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    # Les arguments de classe permettent l'héritage de classes de sklearn\n",
    "    def __init__(self, alpha=1.0):\n",
    "        # alpha est un paramètre pour le lissage: il correspond à la valeur ligne 10 de l'algorithme d'entraînement\n",
    "        # Dans l'algorithme d'entraînement, et comme valeur par défaut, on utilise alpha = 1\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        classes, class_counts = np.unique(y, return_counts=True)\n",
    "        self.priors = class_counts / len(y)\n",
    "        \n",
    "        self.cond_prob = np.zeros((X.shape[1], len(classes)))\n",
    "        for c in classes:\n",
    "            X_c = X[np.where(y == c)]\n",
    "            token_counts = np.sum(X_c, axis=0) + self.alpha\n",
    "            total_count = np.sum(token_counts)\n",
    "            self.cond_prob[:, c] = token_counts / total_count\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):        \n",
    "        scores = np.dot(X, np.log(self.cond_prob))\n",
    "        scores += np.log(self.priors)\n",
    "        \n",
    "        return np.argmax(scores, axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expérimentation\n",
    "\n",
    "On utilise la moitié des données pour l'entraînement, l'autre pour tester le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, X = count_words(texts_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821\n"
     ]
    }
   ],
   "source": [
    "nb = NB()\n",
    "nb.fit(X[::2], y_reduced[::2])\n",
    "print(nb.score(X[1::2], y_reduced[1::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation \n",
    "\n",
    "Avec la fonction *cross_val_score* de scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.8099999999999999 (std 0.010839741694339374)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluer les performances: \n",
    "\n",
    "**Quelles sont les points forts et les points faibles de ce système ? Comment y remédier ?**\n",
    "\n",
    "**Commentaire:**\n",
    "\n",
    "Le systeme du classifieur de bayes naif est avantageux dans le sens où il n'est pas \"cher\" à mettre en place: l'entraînement est rapide, il n'y a qu'un seul passage sur les données pour calculer les probabilités conditionnelles et priors. On peut donc facilement obtenir, pour autant qu'on ait suffisamment de données, un classifieur avec performances raisonnables. Il en va de même pour la prédiction. De plus, on peut avoir accès et lire ces probabilités calculées ce qui offre de l'interprétabilité pour les résultats car on peut analyser les caractéristiques qui les influencent.\n",
    "\n",
    "Cependant, la précision du classifieur de bayes est généralement moins bonne que celle qu'on peut obtenir avec d'autres algorithmes plus complexes. On peut essayer d'améliorer le classifieur en améliorant la représentation des données pour l'entrainement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour aller plus loin: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Améliorer les représentations\n",
    "\n",
    "On utilise la function \n",
    "```python\n",
    "CountVectorizer\n",
    "``` \n",
    "de scikit-learn pour constituer notre corpus. Elle nous permettra d'améliorer facilement nos représentations BoW.\n",
    "\n",
    "#### Tf-idf:\n",
    "\n",
    "Il s'agit du produit de la fréquence du terme (TF) et de sa fréquence inverse dans les documents (IDF).\n",
    "Cette méthode est habituellement utilisée pour extraire l'importance d'un terme $i$ dans un document $j$ relativement au reste du corpus, à partir d'une matrice d'occurences $mots \\times documents$. Ainsi, pour une matrice $\\mathbf{T}$ de $|V|$ termes et $D$ documents:\n",
    "$$\\text{TF}(T, w, d) = \\frac{T_{w,d}}{\\sum_{w'=1}^{|V|} T_{w',d}} $$\n",
    "\n",
    "$$\\text{IDF}(T, w) = \\log\\left(\\frac{D}{|\\{d : T_{w,d} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\text{TF-IDF}(T, w, d) = \\text{TF}(X, w, d) \\cdot \\text{IDF}(T, w)$$\n",
    "\n",
    "On peut l'adapter à notre cas en considérant que le contexte du deuxième mot est le document. Cependant, TF-IDF est généralement plus adaptée aux matrices peu denses, puisque cette mesure pénalisera les termes qui apparaissent dans une grande partie des documents. \n",
    "    \n",
    "#### Ne pas prendre en compte les mots trop fréquents:\n",
    "\n",
    "On peut utiliser l'option \n",
    "```python\n",
    "max_df=1.0\n",
    "```\n",
    "pour modifier la quantité de mots pris en compte. \n",
    "\n",
    "#### Essayer différentes granularités:\n",
    "\n",
    "Plutôt que de simplement compter les mots, on peut compter les séquences de mots - de taille limitée, bien sur. \n",
    "On appelle une séquence de $n$ mots un $n$-gram: essayons d'utiliser les 2 et 3-grams (bi- et trigrams).\n",
    "On peut aussi tenter d'utiliser les séquences de caractères à la place de séquences de mots.\n",
    "\n",
    "On s'intéressera aux options \n",
    "```python\n",
    "analyzer='word'\n",
    "```\n",
    "et \n",
    "```python\n",
    "ngram_range=(1, 2)\n",
    "```\n",
    "que l'on changera pour modifier la granularité. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: 0.812 (std 0.012884098726725092)\n",
      "Classification score tf-idf: 0.8125 (std 0.010488088481701477)\n",
      "Classification score sans mots fréquents: 0.8115 (std 0.011789826122551566)\n",
      "Classification score bigram: 0.8305 (std 0.009273618495495711)\n",
      "Classification score trigram: 0.8225 (std 0.017320508075688742)\n",
      "Classification score char: 0.6094999999999999 (std 0.018261982367749664)\n"
     ]
    }
   ],
   "source": [
    "## On peut définir une pipeline que l'on modifiera pour expérimenter.\n",
    "\n",
    "pipeline_base = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_base, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score: %s (std %s)\" % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "pipeline_tf_idf = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
    "    ('tf_idf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_tf_idf, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score tf-idf: %s (std %s)\" % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "pipeline_maxdf = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None, max_df=0.85)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_maxdf, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score sans mots fréquents: %s (std %s)\" % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "pipeline_bigram = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None, ngram_range=(1, 2))),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_bigram, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score bigram: %s (std %s)\" % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "pipeline_trigram = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None, ngram_range=(1, 3))),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_trigram, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score trigram: %s (std %s)\" % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "pipeline_char = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', stop_words=None)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_char, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score char: %s (std %s)\" % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentaire**:\n",
    "\n",
    "Avec un entrainement sur l'ensemble des 2000 documents, on constate que la précision est légèrement meilleure pour une réprésentation des données avec le TF-IDF, ou bien en prenant en compte les bi-grames ou tri-grames. Ce n'est pas vraiment le cas pour une réprésentation des données avec les charactères seulement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming \n",
    "\n",
    "Permet de revenir à la racine d'un mot: on peut ainsi grouper différents mots autour de la même racine, ce qui facilite la généralisation. Utiliser:\n",
    "```python\n",
    "from nltk import SnowballStemmer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple d'utilisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word : singers ; stemmed : singer\n",
      "word : cat ; stemmed : cat\n",
      "word : generalization ; stemmed : general\n",
      "word : philosophy ; stemmed : philosophi\n",
      "word : psychology ; stemmed : psycholog\n",
      "word : philosopher ; stemmed : philosoph\n"
     ]
    }
   ],
   "source": [
    "words = ['singers', 'cat', 'generalization', 'philosophy', 'psychology', 'philosopher']\n",
    "for word in words:\n",
    "    print('word : %s ; stemmed : %s' %(word, stemmer.stem(word)))#.decode('utf-8'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation des données:\n",
    "\n",
    "Classe vide : function **à compléter** \n",
    "```python\n",
    "def stem(X)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(X):\n",
    "    X_stem = []\n",
    "    for text in X:\n",
    "        text_list = text.split()\n",
    "        text_stem = []\n",
    "        for word in text_list:\n",
    "            text_stem.append(stemmer.stem(word))\n",
    "        X_stem.append(' '.join(text_stem))\n",
    "\n",
    "    return X_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.8089999999999999 (std 0.015215124054702916)\n"
     ]
    }
   ],
   "source": [
    "texts_stemmed = stem(texts_reduced)\n",
    "voc, X = count_words(texts_stemmed)\n",
    "nb = NB()\n",
    "\n",
    "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie du discours\n",
    "\n",
    "Pour généraliser, on peut aussi utiliser les parties du discours (Part of Speech, POS) des mots, ce qui nous permettra  de filtrer l'information qui n'est potentiellement pas utile au modèle. On va récupérer les POS des mots à l'aide des fonctions:\n",
    "```python\n",
    "from nltk import pos_tag, word_tokenize\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple d'utilisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('am', 'VBP'), ('Sam', 'NNP')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pos_tag(word_tokenize(('I am Sam')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Détails des significations des tags POS: https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Transformation des données:\n",
    "\n",
    "Classe vide : fonction **à compléter** \n",
    "```python\n",
    "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB'])\n",
    "``` \n",
    "\n",
    "Ne garder que les noms, adverbes, verbes et adjectifs pour notre modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB']):\n",
    "    X_pos = []\n",
    "    for text in X:\n",
    "        text_pos = pos_tag(word_tokenize(text))\n",
    "        text_pos_filtered = []\n",
    "        for word_pos in text_pos:\n",
    "            if word_pos[1] in good_tags:\n",
    "                text_pos_filtered.append(word_pos[0])\n",
    "        X_pos.append(' '.join(text_pos_filtered))\n",
    "        \n",
    "    return X_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.7825000000000001 (std 0.015411035007422467)\n"
     ]
    }
   ],
   "source": [
    "texts_POS = pos_tag_filter(texts_reduced)\n",
    "voc, X = count_words(texts_POS)\n",
    "nb = NB()\n",
    "\n",
    "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-words \n",
    "\n",
    "Les \"stop-words\" sont les mots apparaissant fréquemment dans les données et que l'on juge non représentatifs. On les considère comme du bruit. Une liste de stop-words est disponible dans le fichier *english.stop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " \"a's\",\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def readFile(fileName):\n",
    "    \"\"\"\n",
    "     * Code for reading a file.  you probably don't want to modify anything here, \n",
    "     * unless you don't like the way we segment files.\n",
    "    \"\"\"\n",
    "    contents = []\n",
    "    f = open(fileName)\n",
    "    for line in f:\n",
    "        contents.append(line)\n",
    "    f.close()\n",
    "    result = ('\\n'.join(contents)).split() \n",
    "    return result\n",
    "\n",
    "sw = readFile('english.stop')\n",
    "sw[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Transformation des données:\n",
    "\n",
    "Classe vide : fonction **à compléter** \n",
    "```python\n",
    "def filterStopWords(X)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterStopWords(X):\n",
    "    \"\"\"Filters stop words.\"\"\"\n",
    "    X_filtered = []\n",
    "    for text in X:\n",
    "        text_list = text.split()\n",
    "        text_filtered = []\n",
    "        for word in text_list:\n",
    "            if word not in sw:\n",
    "                text_filtered.append(word)\n",
    "        X_filtered.append(' '.join(text_filtered))\n",
    "        \n",
    "    return X_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de classification: 0.8030000000000002 (std 0.015362291495737214)\n"
     ]
    }
   ],
   "source": [
    "texts_stop = filterStopWords(texts_reduced)\n",
    "voc, X = count_words(texts_stop)\n",
    "nb = NB()\n",
    "\n",
    "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
    "print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentaire**:\n",
    "\n",
    "On obtient pour les 2000 documents de moins bonnes performances avec les techniques de stemming, selection avec POS et les le filtre des stop words. La restriction et compression sur les données qui en ressort est peut être trop conséquente dans ce cas là pour véritablement améliorer les performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Utilisation d'un classifieur plus complexe ?\n",
    "\n",
    "On peut utiliser les implémentations scikit-learn de classifieurs moins naïfs, comme la régression logistique ou les SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas Jacquemin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: 0.841 (std 0.01677796173556255)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas Jacquemin\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: 0.833 (std 0.01623268308074792)\n"
     ]
    }
   ],
   "source": [
    "pipeline_logistic = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_logistic, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score: %s (std %s)\" % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "pipeline_svm = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words=None)),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "scores = cross_val_score(pipeline_svm, texts_reduced, y_reduced, cv=5)\n",
    "print(\"Classification score: %s (std %s)\" % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentaire**:\n",
    "\n",
    "La performance est légèrement meilleure pour ces deux classifieurs plus complexes. Néanmoins, ce sont des modèles discriminatifs là où le classifieur de bayes naif est un modèle génératif. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
