{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Langage Model with PyTroch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A (very small) introduction to pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Tensors are very similar to Numpy arrays, with the added benefit of being usable on GPU. For a short tutorial on various methods to create tensors of particular types, see [this link](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py).\n",
    "The important things to note are that Tensors can be created empty, from lists, and it is very easy to convert a numpy array into a pytorch tensor, and inversely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0])\n",
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.LongTensor(5)\n",
    "b = torch.LongTensor([5])\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([2])\n",
    "b = torch.FloatTensor([3])\n",
    "\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interest in us using Pytorch is the ```autograd``` package. ```torch.Tensor```objects have an attribute ```.requires_grad```; if set as True, it starts to track all operations on it. When you finish your computation, can call ```.backward()``` and all the gradients are computed automatically (and stored in the ```.grad``` attribute).\n",
    "\n",
    "One way to easily cut a tensor from the computational once it is not needed anymore is to use ```.detach()```.\n",
    "More info on automatic differentiation in pytorch on [this link](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.3061,  0.0160,  0.0133],\n",
      "        [-0.3236,  0.3319, -0.5385]], requires_grad=True)\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([ 0.1456, -0.3180], requires_grad=True)\n",
      "Initial loss:  1.1919654607772827\n",
      "dL/dw:  tensor([[ 0.3157,  0.5700,  0.2983],\n",
      "        [ 0.1289, -0.2091, -0.9097]])\n",
      "dL/db:  tensor([-0.0631, -0.3576])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "for name, p in linear.named_parameters():\n",
    "    print(name)\n",
    "    print(p)\n",
    "\n",
    "# Build loss function - Mean Square Error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('Initial loss: ', loss.item())\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after one update:  1.1767386198043823\n"
     ]
    }
   ],
   "source": [
    "# You can perform gradient descent manually, with an in-place update ...\n",
    "linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('Loss after one update: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after two updates:  1.16192626953125\n"
     ]
    }
   ],
   "source": [
    "# Use the optim package to define an Optimizer that will update the weights of the model.\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# By default, gradients are accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "# is called. Before the backward pass, we need to use the optimizer object to zero all of the\n",
    "# gradients.\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Calling the step function on an Optimizer makes an update to its parameters\n",
    "optimizer.step()\n",
    "\n",
    "# Print out the loss after the second step of gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('Loss after two updates: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a ```Dictionary``` class, that we are going to use to create a vocabulary for our text data. The goal here is to have a convenient tool, with easy access to any information we could need:\n",
    "- A python dictionary ```word2idx``` allowing easy transformation of tokenized text into indexes\n",
    "- A list ```idx2word```, allowing us to find the word corresponding to an index (for interpretation and generation)\n",
    "- A python dictionary ```counter``` used to build the vocabulary, that can provide us with frequency information if needed. \n",
    "- The ```total``` count of words in the dictionary.\n",
    "\n",
    "Important: The data that we are going to use are already pre-processed so we don't need to create special tokens and control the size of the vocabulary ourselves. However, when the text data is raw, methods to preprocess it conveniently should be added here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.counter = {}\n",
    "        self.total = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "            self.counter.setdefault(word, 0)\n",
    "        self.counter[word] += 1\n",
    "        self.total += 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      " = Valkyria Chronicles III = \n",
      "\n",
      " \n",
      "\n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./wikitext-2/train.txt', 'r', encoding='utf8') as f:\n",
    "    print(f.readline())\n",
    "    print(f.readline())\n",
    "    print(f.readline())\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\"': 60,\n",
      " '(': 9,\n",
      " ')': 18,\n",
      " ',': 12,\n",
      " '.': 14,\n",
      " '2011': 44,\n",
      " '3': 6,\n",
      " ':': 7,\n",
      " '<unk>': 8,\n",
      " '=': 0,\n",
      " '@-@': 29,\n",
      " 'Battlefield': 17,\n",
      " 'Chronicles': 2,\n",
      " 'Europan': 70,\n",
      " 'Gallia': 67,\n",
      " 'III': 3,\n",
      " 'Imperial': 80,\n",
      " 'January': 43,\n",
      " 'Japan': 24,\n",
      " 'Japanese': 10,\n",
      " 'Media.Vision': 37,\n",
      " 'Nameless': 61,\n",
      " 'PlayStation': 39,\n",
      " 'Portable': 40,\n",
      " 'Raven': 81,\n",
      " 'Released': 41,\n",
      " 'Second': 69,\n",
      " 'Sega': 35,\n",
      " 'Senjō': 4,\n",
      " 'Valkyria': 1,\n",
      " 'War': 71,\n",
      " 'a': 26,\n",
      " 'against': 79,\n",
      " 'and': 36,\n",
      " 'are': 77,\n",
      " 'as': 22,\n",
      " 'black': 75,\n",
      " 'by': 34,\n",
      " 'commonly': 19,\n",
      " 'developed': 33,\n",
      " 'during': 68,\n",
      " 'first': 58,\n",
      " 'follows': 59,\n",
      " 'for': 38,\n",
      " 'fusion': 49,\n",
      " 'game': 32,\n",
      " 'gameplay': 52,\n",
      " 'in': 42,\n",
      " 'is': 25,\n",
      " 'it': 45,\n",
      " 'its': 53,\n",
      " 'lit': 13,\n",
      " 'military': 63,\n",
      " 'nation': 66,\n",
      " 'no': 5,\n",
      " 'of': 15,\n",
      " 'operations': 76,\n",
      " 'outside': 23,\n",
      " 'parallel': 57,\n",
      " 'penal': 62,\n",
      " 'perform': 73,\n",
      " 'pitted': 78,\n",
      " 'playing': 30,\n",
      " 'predecessors': 54,\n",
      " 'real': 50,\n",
      " 'referred': 20,\n",
      " 'role': 28,\n",
      " 'runs': 56,\n",
      " 'same': 48,\n",
      " 'secret': 74,\n",
      " 'series': 47,\n",
      " 'serving': 65,\n",
      " 'story': 55,\n",
      " 'tactical': 27,\n",
      " 'the': 16,\n",
      " 'third': 46,\n",
      " 'time': 51,\n",
      " 'to': 21,\n",
      " 'unit': 64,\n",
      " 'video': 31,\n",
      " 'who': 72,\n",
      " '戦場のヴァルキュリア3': 11}\n",
      "['=',\n",
      " 'Valkyria',\n",
      " 'Chronicles',\n",
      " 'III',\n",
      " 'Senjō',\n",
      " 'no',\n",
      " '3',\n",
      " ':',\n",
      " '<unk>',\n",
      " '(',\n",
      " 'Japanese',\n",
      " '戦場のヴァルキュリア3',\n",
      " ',',\n",
      " 'lit',\n",
      " '.',\n",
      " 'of',\n",
      " 'the',\n",
      " 'Battlefield',\n",
      " ')',\n",
      " 'commonly',\n",
      " 'referred',\n",
      " 'to',\n",
      " 'as',\n",
      " 'outside',\n",
      " 'Japan',\n",
      " 'is',\n",
      " 'a',\n",
      " 'tactical',\n",
      " 'role',\n",
      " '@-@',\n",
      " 'playing',\n",
      " 'video',\n",
      " 'game',\n",
      " 'developed',\n",
      " 'by',\n",
      " 'Sega',\n",
      " 'and',\n",
      " 'Media.Vision',\n",
      " 'for',\n",
      " 'PlayStation',\n",
      " 'Portable',\n",
      " 'Released',\n",
      " 'in',\n",
      " 'January',\n",
      " '2011',\n",
      " 'it',\n",
      " 'third',\n",
      " 'series',\n",
      " 'same',\n",
      " 'fusion',\n",
      " 'real',\n",
      " 'time',\n",
      " 'gameplay',\n",
      " 'its',\n",
      " 'predecessors',\n",
      " 'story',\n",
      " 'runs',\n",
      " 'parallel',\n",
      " 'first',\n",
      " 'follows',\n",
      " '\"',\n",
      " 'Nameless',\n",
      " 'penal',\n",
      " 'military',\n",
      " 'unit',\n",
      " 'serving',\n",
      " 'nation',\n",
      " 'Gallia',\n",
      " 'during',\n",
      " 'Second',\n",
      " 'Europan',\n",
      " 'War',\n",
      " 'who',\n",
      " 'perform',\n",
      " 'secret',\n",
      " 'black',\n",
      " 'operations',\n",
      " 'are',\n",
      " 'pitted',\n",
      " 'against',\n",
      " 'Imperial',\n",
      " 'Raven']\n",
      "{'\"': 4,\n",
      " '(': 1,\n",
      " ')': 1,\n",
      " ',': 6,\n",
      " '.': 4,\n",
      " '2011': 1,\n",
      " '3': 2,\n",
      " ':': 2,\n",
      " '<unk>': 3,\n",
      " '=': 2,\n",
      " '@-@': 2,\n",
      " 'Battlefield': 1,\n",
      " 'Chronicles': 3,\n",
      " 'Europan': 1,\n",
      " 'Gallia': 1,\n",
      " 'III': 2,\n",
      " 'Imperial': 1,\n",
      " 'January': 1,\n",
      " 'Japan': 2,\n",
      " 'Japanese': 1,\n",
      " 'Media.Vision': 1,\n",
      " 'Nameless': 1,\n",
      " 'PlayStation': 1,\n",
      " 'Portable': 1,\n",
      " 'Raven': 1,\n",
      " 'Released': 1,\n",
      " 'Second': 1,\n",
      " 'Sega': 1,\n",
      " 'Senjō': 1,\n",
      " 'Valkyria': 5,\n",
      " 'War': 1,\n",
      " 'a': 2,\n",
      " 'against': 1,\n",
      " 'and': 4,\n",
      " 'are': 1,\n",
      " 'as': 2,\n",
      " 'black': 1,\n",
      " 'by': 1,\n",
      " 'commonly': 1,\n",
      " 'developed': 1,\n",
      " 'during': 1,\n",
      " 'first': 1,\n",
      " 'follows': 1,\n",
      " 'for': 1,\n",
      " 'fusion': 1,\n",
      " 'game': 3,\n",
      " 'gameplay': 1,\n",
      " 'in': 3,\n",
      " 'is': 2,\n",
      " 'it': 1,\n",
      " 'its': 1,\n",
      " 'lit': 1,\n",
      " 'military': 1,\n",
      " 'nation': 1,\n",
      " 'no': 1,\n",
      " 'of': 3,\n",
      " 'operations': 1,\n",
      " 'outside': 1,\n",
      " 'parallel': 1,\n",
      " 'penal': 1,\n",
      " 'perform': 1,\n",
      " 'pitted': 1,\n",
      " 'playing': 1,\n",
      " 'predecessors': 1,\n",
      " 'real': 1,\n",
      " 'referred': 1,\n",
      " 'role': 1,\n",
      " 'runs': 1,\n",
      " 'same': 1,\n",
      " 'secret': 1,\n",
      " 'series': 1,\n",
      " 'serving': 1,\n",
      " 'story': 1,\n",
      " 'tactical': 2,\n",
      " 'the': 11,\n",
      " 'third': 1,\n",
      " 'time': 1,\n",
      " 'to': 2,\n",
      " 'unit': 2,\n",
      " 'video': 1,\n",
      " 'who': 1,\n",
      " '戦場のヴァルキュリア3': 1}\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "# Let's take the four first lines of our training data:\n",
    "corpus = ''\n",
    "with open('./wikitext-2/train.txt', 'r', encoding='utf8') as f:\n",
    "    for i in range(4):\n",
    "        corpus += f.readline()\n",
    "        \n",
    "# Create an empty Dictionary, separate and add all words. \n",
    "dictio = Dictionary()\n",
    "words = corpus.split()\n",
    "for word in words:\n",
    "    dictio.add_word(word)\n",
    "\n",
    "# Take a look at the objects created:\n",
    "pp.pprint(dictio.word2idx)\n",
    "pp.pprint(dictio.idx2word)\n",
    "pp.pprint(dictio.counter)\n",
    "pp.pprint(dictio.total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        # We create an object Dictionary associated to Corpus\n",
    "        self.dictionary = Dictionary()\n",
    "        # We go through all files, adding all words to the dictionary\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "        \n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file, knowing the dictionary, in order to tranform it into a list of indexes\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "                tokens += len(words)\n",
    "        \n",
    "        # Once done, go through the file a second time and fill a Torch Tensor with the associated indexes \n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "data = './wikitext-2-small/'\n",
    "corpus = Corpus(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383196\n",
      "19482\n",
      "19482\n",
      "torch.Size([275485])\n",
      "tensor([0, 1, 2, 3, 4, 1, 0])\n",
      "['<eos>', '=', 'Valkyria', 'Chronicles', 'III', '=', '<eos>']\n",
      "torch.Size([47945])\n",
      "tensor([    0,     1, 17642, 17643,     1,     0,     0])\n",
      "['<eos>', '=', 'Homarus', 'gammarus', '=', '<eos>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(corpus.dictionary.total)\n",
    "print(len(corpus.dictionary.idx2word))\n",
    "print(len(corpus.dictionary.word2idx))\n",
    "\n",
    "print(corpus.train.shape)\n",
    "print(corpus.train[0:7])\n",
    "print([corpus.dictionary.idx2word[corpus.train[i]] for i in range(7)])\n",
    "\n",
    "print(corpus.valid.shape)\n",
    "print(corpus.valid[0:7])\n",
    "print([corpus.dictionary.idx2word[corpus.valid[i]] for i in range(7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have data under a very long list of indexes: the text is as one sequence.\n",
    "# The idea now is to create batches from this. Note that this is absolutely not the best\n",
    "# way to proceed with large quantities of data (where we'll try not to store huge tensors\n",
    "# in memory but read them from file as we go) !\n",
    "# Here, we are looking for simplicity and efficiency with regards to computation time.\n",
    "# That is why we will ignore sentence separations and treat the data as one long stream that\n",
    "# we will cut arbitrarily as we need.\n",
    "# With the alphabet being our data, we currently have the sequence:\n",
    "# [a b c d e f g h i j k l m n o p q r s t u v w x y z]\n",
    "# We want to reorganize it as independant batches that will be processed independantly by the model !\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get the 4 following sequences:\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘\n",
    "# with the last two elements being lost.\n",
    "# Again, these columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient processing.\n",
    "\n",
    "def batchify(data, batch_size, cuda = False):\n",
    "    # Cut the elements that are unnecessary\n",
    "    nbatch = data.size(0) // batch_size\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # Reorganize the data\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    # If we can use a GPU, let's tranfer the tensor to it\n",
    "    if cuda:\n",
    "        data = data.cuda()\n",
    "    return data\n",
    "\n",
    "# get_batch subdivides the source data into chunks of the appropriate length.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a sequence length (seq_len) of 3, we'd get the following two variables:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# │ b h n t │ │ c i o u │\n",
    "# └ c i o u ┘ └ d j p v ┘\n",
    "# The first variable contains the letters input to the network, while the second\n",
    "# contains the one we want the network to predict (b for a, h for g, v for u, etc..)\n",
    "# Note that despite the name of the function, we are cutting the data in the\n",
    "# temporal dimension, since we already divided data into batches in the previous\n",
    "# function. \n",
    "\n",
    "def get_batch(source, i, seq_len, evaluation=False):\n",
    "    # Deal with the possibility that there's not enough data left for a full sequence\n",
    "    seq_len = min(seq_len, len(source) - 1 - i)\n",
    "    # Take the input data\n",
    "    data = source[i:i+seq_len]\n",
    "    # Shift by one for the target data\n",
    "    target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2754, 100])\n",
      "torch.Size([11986, 4])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "eval_batch_size = 4\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,    10,    15,    91],\n",
      "        [    1,  3018,   735,    13],\n",
      "        [17642,   187,   766,   496]])\n",
      "tensor([[    1,  3018,   735,    13],\n",
      "        [17642,   187,   766,   496],\n",
      "        [17643,   827,   751,   131]])\n",
      "tensor([[17643,   827,   751,   131],\n",
      "        [    1,    19,  4659,  2200],\n",
      "        [    0,    17,  2466,    22]])\n",
      "tensor([[   1,   19, 4659, 2200],\n",
      "        [   0,   17, 2466,   22],\n",
      "        [   0, 3069,   39, 5521]])\n"
     ]
    }
   ],
   "source": [
    "input_words, target_words = get_batch(val_data, 0, 3)\n",
    "pp.pprint(input_words)\n",
    "pp.pprint(target_words)\n",
    "input_words, target_words = get_batch(val_data, 3, 3)\n",
    "pp.pprint(input_words)\n",
    "pp.pprint(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cells in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0867,  0.0207,  0.3831]],\n",
      "\n",
      "        [[-0.1501,  0.0348,  0.2018]],\n",
      "\n",
      "        [[-0.1560,  0.0125,  0.2985]],\n",
      "\n",
      "        [[-0.1142, -0.0208,  0.2156]],\n",
      "\n",
      "        [[-0.1934, -0.0381,  0.0406]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.1934, -0.0381,  0.0406]]], grad_fn=<StackBackward>),\n",
      " tensor([[[-0.6343, -0.1229,  0.0934]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "# Create a toy example of LSTM: \n",
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# LSTMs expect inputs having 3 dimensions:\n",
    "# - The first dimension is the temporal dimension, along which we (in our case) have the different words\n",
    "# - The second dimension is the batch dimension, along which we stack the independant batches\n",
    "# - The third dimension is the feature dimension, along which are the features of the vector representing the words\n",
    "\n",
    "# In our toy case, we have inputs and outputs containing 3 features (third dimension !)\n",
    "# We created a sequence of 5 different inputs (first dimension !)\n",
    "# We don't use batch (the second dimension will have one lement)\n",
    "\n",
    "# We need an initial hidden state, of the right sizes for dimension 2/3, but with only one temporal element:\n",
    "# Here, it is:\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "# Why do we create a tuple of two tensors ? Because we use LSTMs: remember that they use two sets of weights,\n",
    "# and two hidden states (Hidden state, and Cell state).\n",
    "# If you don't remember, read: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "# If we used a classic RNN, we would simply have:\n",
    "# hidden = torch.randn(1, 1, 3)\n",
    "\n",
    "# The naive way of applying a lstm to inputs is to apply it one step at a time, and loop through the sequence\n",
    "for i in inputs:\n",
    "    # After each step, hidden contains the hidden states (remember, it's a tuple of two states).\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    \n",
    "# Alternatively, we can do the entire sequence all at once.\n",
    "# The first value returned by LSTM is all of the Hidden states throughout the sequence.\n",
    "# The second is just the most recent Hidden state and Cell state (you can compare the values)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence, for each temporal step\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate later, with another sequence\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # Re-initialize\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "pp.pprint(out)\n",
    "pp.pprint(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our own LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models are usually implemented as custom nn.Module subclass\n",
    "# We need to redefine the __init__ method, which creates the object\n",
    "# We also need to redefine the forward method, which transform the input into outputs\n",
    "# We can also add any method that we need: here, in order to initiate weights in the model\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Create a dropout object to use on layers for regularization\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        # Create an encoder - which is an embedding layer\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        # Create the LSTM layers - find out how to stack them !\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        # Create what we call the decoder: a linear transformation to map the hidden state into scores for all words in the vocabulary\n",
    "        # (Note that the softmax application function will be applied out of the model)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        \n",
    "        # Initialize non-reccurent weights \n",
    "        self.init_weights()\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # Initialize the encoder and decoder weights with the uniform distribution\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize the hidden state and cell state to zero, with the right sizes\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, batch_size, self.nhid))    \n",
    "\n",
    "    def forward(self, input, hidden, return_h=False):\n",
    "        # Process the input\n",
    "        emb = self.drop(self.encoder(input))   \n",
    "        \n",
    "        # Apply the LSTMs\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        # Decode into scores\n",
    "        output = self.drop(output)      \n",
    "        decoded = self.decoder(output)\n",
    "        return decoded, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You have a CUDA device, so you should probably choose cuda = True\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# If you have Cuda installed and a GPU available\n",
    "cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    if not cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably choose cuda = True\")\n",
    "        \n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "hidden_size = 200\n",
    "layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "vocab_size = len(corpus.dictionary)\n",
    "model = LSTMModel(vocab_size, embedding_size, hidden_size, layers, dropout).to(device)\n",
    "params = list(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 10.0\n",
    "optimizer = 'sgd'\n",
    "wdecay = 1.2e-6\n",
    "# For gradient clipping\n",
    "clip = 0.25\n",
    "\n",
    "if optimizer == 'sgd':\n",
    "    optim = torch.optim.SGD(params, lr=lr, weight_decay=wdecay)\n",
    "if optimizer == 'adam':\n",
    "    optim = torch.optim.Adam(params, lr=lr, weight_decay=wdecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's think about gradient propagation:\n",
    "# We plan to keep the second ouput of the LSTM layer (the hidden/cell states) to initialize\n",
    "# the next call to LSTM. In this way, we can back-propagate the gradient for as long as we want.\n",
    "# However, this put a huge strain on the memory used by the model, since it implies retaining\n",
    "# a always-growing number of tensors of gradients in the cache.\n",
    "# We decide to not backpropagate through time beyond the current sequence ! \n",
    "# We use a specific function to cut the 'hidden/state cell' states from their previous dependencies\n",
    "# before using them to initialize the next call to the LSTM.\n",
    "# This is done with the .detach() function.\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other global parameters\n",
    "epochs = 10\n",
    "seq_len = 30\n",
    "log_interval = 10\n",
    "save = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, seq_len):\n",
    "            data, targets = get_batch(data_source, i, seq_len)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            total_loss += len(data) * criterion(output.view(-1, vocab_size), targets.view(-1)).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
    "        data, targets = get_batch(train_data, i, seq_len)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(params, clip)\n",
    "        optim.step()\n",
    "        \n",
    "        total_loss += loss.data\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // seq_len, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    10/   91 batches | lr 10.00 | ms/batch 1398.76 | loss  9.82 | ppl 18312.55\n",
      "| epoch   1 |    20/   91 batches | lr 10.00 | ms/batch 1271.10 | loss  8.12 | ppl  3356.70\n",
      "| epoch   1 |    30/   91 batches | lr 10.00 | ms/batch 1280.87 | loss  7.75 | ppl  2311.62\n",
      "| epoch   1 |    40/   91 batches | lr 10.00 | ms/batch 1272.00 | loss  7.60 | ppl  2003.75\n",
      "| epoch   1 |    50/   91 batches | lr 10.00 | ms/batch 1263.72 | loss  7.52 | ppl  1839.20\n",
      "| epoch   1 |    60/   91 batches | lr 10.00 | ms/batch 1272.20 | loss  7.47 | ppl  1749.20\n",
      "| epoch   1 |    70/   91 batches | lr 10.00 | ms/batch 1267.61 | loss  7.39 | ppl  1613.79\n",
      "| epoch   1 |    80/   91 batches | lr 10.00 | ms/batch 1265.82 | loss  7.40 | ppl  1634.01\n",
      "| epoch   1 |    90/   91 batches | lr 10.00 | ms/batch 1266.71 | loss  7.36 | ppl  1573.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 140.33s | valid loss  7.25 | valid ppl  1401.52\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas Jacquemin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTMModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\Thomas Jacquemin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\Thomas Jacquemin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\Thomas Jacquemin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\Thomas Jacquemin\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |    10/   91 batches | lr 10.00 | ms/batch 1381.11 | loss  8.02 | ppl  3052.86\n",
      "| epoch   2 |    20/   91 batches | lr 10.00 | ms/batch 1276.49 | loss  7.31 | ppl  1489.99\n",
      "| epoch   2 |    30/   91 batches | lr 10.00 | ms/batch 1269.51 | loss  7.27 | ppl  1431.21\n",
      "| epoch   2 |    40/   91 batches | lr 10.00 | ms/batch 1283.17 | loss  7.24 | ppl  1387.80\n",
      "| epoch   2 |    50/   91 batches | lr 10.00 | ms/batch 1255.34 | loss  7.16 | ppl  1289.80\n",
      "| epoch   2 |    60/   91 batches | lr 10.00 | ms/batch 1263.02 | loss  7.12 | ppl  1234.17\n",
      "| epoch   2 |    70/   91 batches | lr 10.00 | ms/batch 1258.34 | loss  7.09 | ppl  1203.19\n",
      "| epoch   2 |    80/   91 batches | lr 10.00 | ms/batch 1262.22 | loss  7.08 | ppl  1185.89\n",
      "| epoch   2 |    90/   91 batches | lr 10.00 | ms/batch 1257.74 | loss  7.07 | ppl  1177.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 140.09s | valid loss  6.96 | valid ppl  1048.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    10/   91 batches | lr 10.00 | ms/batch 1399.26 | loss  7.72 | ppl  2246.15\n",
      "| epoch   3 |    20/   91 batches | lr 10.00 | ms/batch 1267.41 | loss  7.01 | ppl  1108.08\n",
      "| epoch   3 |    30/   91 batches | lr 10.00 | ms/batch 1365.55 | loss  6.95 | ppl  1038.75\n",
      "| epoch   3 |    40/   91 batches | lr 10.00 | ms/batch 1328.33 | loss  6.95 | ppl  1043.87\n",
      "| epoch   3 |    50/   91 batches | lr 10.00 | ms/batch 1287.47 | loss  6.89 | ppl   981.51\n",
      "| epoch   3 |    60/   91 batches | lr 10.00 | ms/batch 1257.74 | loss  6.93 | ppl  1018.21\n",
      "| epoch   3 |    70/   91 batches | lr 10.00 | ms/batch 1262.92 | loss  6.89 | ppl   982.76\n",
      "| epoch   3 |    80/   91 batches | lr 10.00 | ms/batch 1249.36 | loss  6.85 | ppl   942.18\n",
      "| epoch   3 |    90/   91 batches | lr 10.00 | ms/batch 1270.20 | loss  6.83 | ppl   926.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 141.44s | valid loss  6.62 | valid ppl   746.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    10/   91 batches | lr 10.00 | ms/batch 1384.90 | loss  7.47 | ppl  1752.62\n",
      "| epoch   4 |    20/   91 batches | lr 10.00 | ms/batch 1253.45 | loss  6.81 | ppl   905.86\n",
      "| epoch   4 |    30/   91 batches | lr 10.00 | ms/batch 1255.44 | loss  6.77 | ppl   867.65\n",
      "| epoch   4 |    40/   91 batches | lr 10.00 | ms/batch 1260.83 | loss  6.76 | ppl   859.83\n",
      "| epoch   4 |    50/   91 batches | lr 10.00 | ms/batch 1253.35 | loss  6.70 | ppl   811.93\n",
      "| epoch   4 |    60/   91 batches | lr 10.00 | ms/batch 1246.87 | loss  6.73 | ppl   837.75\n",
      "| epoch   4 |    70/   91 batches | lr 10.00 | ms/batch 1253.45 | loss  6.71 | ppl   819.80\n",
      "| epoch   4 |    80/   91 batches | lr 10.00 | ms/batch 1247.86 | loss  6.69 | ppl   803.84\n",
      "| epoch   4 |    90/   91 batches | lr 10.00 | ms/batch 1246.67 | loss  6.72 | ppl   828.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 138.61s | valid loss  6.51 | valid ppl   670.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    10/   91 batches | lr 10.00 | ms/batch 1385.50 | loss  7.29 | ppl  1468.46\n",
      "| epoch   5 |    20/   91 batches | lr 10.00 | ms/batch 1259.23 | loss  6.64 | ppl   764.41\n",
      "| epoch   5 |    30/   91 batches | lr 10.00 | ms/batch 1247.17 | loss  6.62 | ppl   751.39\n",
      "| epoch   5 |    40/   91 batches | lr 10.00 | ms/batch 1255.84 | loss  6.61 | ppl   739.13\n",
      "| epoch   5 |    50/   91 batches | lr 10.00 | ms/batch 1250.66 | loss  6.57 | ppl   709.96\n",
      "| epoch   5 |    60/   91 batches | lr 10.00 | ms/batch 1245.77 | loss  6.61 | ppl   745.48\n",
      "| epoch   5 |    70/   91 batches | lr 10.00 | ms/batch 1257.04 | loss  6.57 | ppl   712.13\n",
      "| epoch   5 |    80/   91 batches | lr 10.00 | ms/batch 1252.45 | loss  6.57 | ppl   713.23\n",
      "| epoch   5 |    90/   91 batches | lr 10.00 | ms/batch 1242.68 | loss  6.55 | ppl   696.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 138.66s | valid loss  6.49 | valid ppl   659.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    10/   91 batches | lr 10.00 | ms/batch 1384.40 | loss  7.17 | ppl  1294.36\n",
      "| epoch   6 |    20/   91 batches | lr 10.00 | ms/batch 1245.57 | loss  6.53 | ppl   688.77\n",
      "| epoch   6 |    30/   91 batches | lr 10.00 | ms/batch 1248.06 | loss  6.51 | ppl   674.12\n",
      "| epoch   6 |    40/   91 batches | lr 10.00 | ms/batch 1251.85 | loss  6.49 | ppl   659.81\n",
      "| epoch   6 |    50/   91 batches | lr 10.00 | ms/batch 1255.04 | loss  6.49 | ppl   659.36\n",
      "| epoch   6 |    60/   91 batches | lr 10.00 | ms/batch 1247.96 | loss  6.49 | ppl   657.54\n",
      "| epoch   6 |    70/   91 batches | lr 10.00 | ms/batch 1251.35 | loss  6.42 | ppl   611.47\n",
      "| epoch   6 |    80/   91 batches | lr 10.00 | ms/batch 1232.90 | loss  6.47 | ppl   644.00\n",
      "| epoch   6 |    90/   91 batches | lr 10.00 | ms/batch 1239.49 | loss  6.47 | ppl   647.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 138.33s | valid loss  6.33 | valid ppl   562.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    10/   91 batches | lr 10.00 | ms/batch 1370.14 | loss  7.06 | ppl  1163.30\n",
      "| epoch   7 |    20/   91 batches | lr 10.00 | ms/batch 1250.06 | loss  6.44 | ppl   624.59\n",
      "| epoch   7 |    30/   91 batches | lr 10.00 | ms/batch 1258.63 | loss  6.38 | ppl   590.52\n",
      "| epoch   7 |    40/   91 batches | lr 10.00 | ms/batch 1258.83 | loss  6.42 | ppl   613.95\n",
      "| epoch   7 |    50/   91 batches | lr 10.00 | ms/batch 1240.98 | loss  6.36 | ppl   575.99\n",
      "| epoch   7 |    60/   91 batches | lr 10.00 | ms/batch 1247.36 | loss  6.40 | ppl   600.07\n",
      "| epoch   7 |    70/   91 batches | lr 10.00 | ms/batch 1244.57 | loss  6.36 | ppl   577.69\n",
      "| epoch   7 |    80/   91 batches | lr 10.00 | ms/batch 1262.72 | loss  6.37 | ppl   583.73\n",
      "| epoch   7 |    90/   91 batches | lr 10.00 | ms/batch 1258.24 | loss  6.36 | ppl   579.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 138.91s | valid loss  6.29 | valid ppl   538.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    10/   91 batches | lr 10.00 | ms/batch 1388.09 | loss  6.95 | ppl  1044.02\n",
      "| epoch   8 |    20/   91 batches | lr 10.00 | ms/batch 1253.95 | loss  6.36 | ppl   576.12\n",
      "| epoch   8 |    30/   91 batches | lr 10.00 | ms/batch 1246.57 | loss  6.33 | ppl   558.83\n",
      "| epoch   8 |    40/   91 batches | lr 10.00 | ms/batch 1257.04 | loss  6.32 | ppl   555.73\n",
      "| epoch   8 |    50/   91 batches | lr 10.00 | ms/batch 1248.16 | loss  6.29 | ppl   539.38\n",
      "| epoch   8 |    60/   91 batches | lr 10.00 | ms/batch 1254.45 | loss  6.30 | ppl   543.99\n",
      "| epoch   8 |    70/   91 batches | lr 10.00 | ms/batch 1253.95 | loss  6.27 | ppl   530.46\n",
      "| epoch   8 |    80/   91 batches | lr 10.00 | ms/batch 1250.26 | loss  6.27 | ppl   528.21\n",
      "| epoch   8 |    90/   91 batches | lr 10.00 | ms/batch 1251.65 | loss  6.30 | ppl   542.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 139.03s | valid loss  6.29 | valid ppl   537.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    10/   91 batches | lr 10.00 | ms/batch 1378.81 | loss  6.90 | ppl   987.74\n",
      "| epoch   9 |    20/   91 batches | lr 10.00 | ms/batch 1250.16 | loss  6.27 | ppl   526.72\n",
      "| epoch   9 |    30/   91 batches | lr 10.00 | ms/batch 1234.70 | loss  6.21 | ppl   497.76\n",
      "| epoch   9 |    40/   91 batches | lr 10.00 | ms/batch 1250.26 | loss  6.24 | ppl   514.36\n",
      "| epoch   9 |    50/   91 batches | lr 10.00 | ms/batch 1248.26 | loss  6.19 | ppl   487.99\n",
      "| epoch   9 |    60/   91 batches | lr 10.00 | ms/batch 1247.66 | loss  6.21 | ppl   497.97\n",
      "| epoch   9 |    70/   91 batches | lr 10.00 | ms/batch 1245.67 | loss  6.18 | ppl   484.94\n",
      "| epoch   9 |    80/   91 batches | lr 10.00 | ms/batch 1240.88 | loss  6.21 | ppl   500.02\n",
      "| epoch   9 |    90/   91 batches | lr 10.00 | ms/batch 1247.96 | loss  6.23 | ppl   507.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 137.99s | valid loss  6.14 | valid ppl   464.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    10/   91 batches | lr 10.00 | ms/batch 1378.71 | loss  6.77 | ppl   869.26\n",
      "| epoch  10 |    20/   91 batches | lr 10.00 | ms/batch 1233.00 | loss  6.18 | ppl   485.06\n",
      "| epoch  10 |    30/   91 batches | lr 10.00 | ms/batch 1248.46 | loss  6.17 | ppl   478.73\n",
      "| epoch  10 |    40/   91 batches | lr 10.00 | ms/batch 1253.65 | loss  6.14 | ppl   465.71\n",
      "| epoch  10 |    50/   91 batches | lr 10.00 | ms/batch 1240.48 | loss  6.11 | ppl   452.54\n",
      "| epoch  10 |    60/   91 batches | lr 10.00 | ms/batch 1249.96 | loss  6.15 | ppl   470.73\n",
      "| epoch  10 |    70/   91 batches | lr 10.00 | ms/batch 1243.77 | loss  6.08 | ppl   438.39\n",
      "| epoch  10 |    80/   91 batches | lr 10.00 | ms/batch 1239.39 | loss  6.14 | ppl   465.42\n",
      "| epoch  10 |    90/   91 batches | lr 10.00 | ms/batch 1249.66 | loss  6.14 | ppl   463.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 138.14s | valid loss  6.09 | valid ppl   443.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  6.14 | test ppl   464.34\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    model.rnn.flatten_parameters()\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
