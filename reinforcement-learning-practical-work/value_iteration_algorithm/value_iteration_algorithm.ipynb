{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='text-align:center'>Value iteration algorithm</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the value iteration algorithm for the following simple Markov Decision Process graph:\n",
    "\n",
    "![MDP Graph](pictures/MDPgraph.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this transition model, there are two possible policies:  \n",
    "\n",
    "$\\pi_1$:  \n",
    "$S_0\\mapsto a_1$  \n",
    "$S_1\\mapsto a_0$  \n",
    "$S_2\\mapsto a_0$  \n",
    "$S_3\\mapsto a_0$  \n",
    "\n",
    "$\\pi_2$:  \n",
    "$S_0\\mapsto a_2$  \n",
    "$S_1\\mapsto a_0$  \n",
    "$S_2\\mapsto a_0$  \n",
    "$S_3\\mapsto a_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation of the optimal value function for a state $S$\n",
    "\n",
    "<p style=\"text-align:center\">$V^*(S)= R(S) + \\gamma\\:\\underset{a}{max}\\sum_{S'}T(S, a, S')V^*(S')$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above formula:  \n",
    "$V^*(S_0) = R(S_0) + \\gamma\\:max\n",
    "\\begin{vmatrix}\n",
    "T(S_0, a_0, S_0)V^*(S_0) + T(S_0, a_0, S_1)V^*(S_1) + T(S_0, a_0, S_2)V^*(S_2) + T(S_0, a_0, S_3)V^*(S_3)\n",
    "\\\\ \n",
    "T(S_0, a_1, S_0)V^*(S_0) + T(S_0, a_1, S_1)V^*(S_1) + T(S_0, a_1, S_2)V^*(S_2) + T(S_0, a_1, S_3)V^*(S_3)\n",
    "\\\\ \n",
    "T(S_0, a_2, S_0)V^*(S_0) + T(S_0, a_2, S_1)V^*(S_1) + T(S_0, a_2, S_2)V^*(S_2) + T(S_0, a_2, S_3)V^*(S_3)\n",
    "\\end{vmatrix}$  \n",
    "  \n",
    "$ = \\gamma\\max[V^*(S_1), V^*(S_2)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way we find:  \n",
    "  \n",
    "$V^*(S_1) = \\gamma[(1-x)V^*(S_1) + xV^*(S_3)]$  \n",
    "  \n",
    "$V^*(S_2) = 1 + \\gamma[(1-y)V^*(S_0) + yV^*(S_3)]$  \n",
    "  \n",
    "$V^*(S_3) = 10 + \\gamma V^*(S_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm implementation using x=y=0.25 and Î³ = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value for each state : [14.18563922942206, 15.761821366024511, 15.697898423817858, 22.767075306479853]\n",
      "Optimal policy for each state : [1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Markov decision process parameters\n",
    "x = 0.25\n",
    "y = 0.25\n",
    "\n",
    "# Value function parameter\n",
    "gamma = 0.9\n",
    "\n",
    "# Values and policies for each state\n",
    "Values = np.zeros(4)\n",
    "Policies = np.zeros(4)\n",
    "\n",
    "# Transition matrix for a0\n",
    "T_a0 = np.zeros((4,4))\n",
    "T_a0[1][1] = 1 - x\n",
    "T_a0[1][3] = x\n",
    "T_a0[2][0] = 1 - y\n",
    "T_a0[2][3] = y\n",
    "T_a0[3][0] = 1\n",
    "\n",
    "# Transition matrix for a1\n",
    "T_a1 = np.zeros((4,4))\n",
    "T_a1[0][1] = 1\n",
    "\n",
    "# Transition matrix for a2\n",
    "T_a2 = np.zeros((4,4))\n",
    "T_a2[0][2] = 1\n",
    "\n",
    "# Rewards for each state\n",
    "Rewards = np.zeros(4)\n",
    "Rewards[3] = 10\n",
    "Rewards[2] = 1\n",
    "\n",
    "# Initialisation of a difference vector to detect values convergence\n",
    "values_dif = np.array([1, 1, 1, 1])\n",
    "\n",
    "# Loop to find the optimal value and policy for each state until convergence\n",
    "while(values_dif.any() != 0):\n",
    "    Values_prev = Values\n",
    "    new_values = []\n",
    "    new_policies = []\n",
    "    \n",
    "    for state_i in range(4):\n",
    "        new_values.append(Rewards[state_i] + gamma * max(np.dot(T_a0[state_i], Values),\n",
    "                                                   np.dot(T_a1[state_i], Values),\n",
    "                                                   np.dot(T_a2[state_i], Values)))\n",
    "        \n",
    "        new_policies.append(np.argmax([np.dot(T_a0[state_i], Values),\n",
    "                                       np.dot(T_a1[state_i], Values),\n",
    "                                       np.dot(T_a2[state_i], Values)]))\n",
    "\n",
    "    values_dif = np.array(new_values) - Values_prev\n",
    "    Policies = new_policies\n",
    "    Values = new_values\n",
    "    \n",
    "print('Optimal value for each state : {}'.format(Values))\n",
    "print('Optimal policy for each state : {}'.format(Policies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
